{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2ae433",
   "metadata": {},
   "source": [
    "## BUILD THE NEURAL NETWORK\n",
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81480b7d",
   "metadata": {},
   "source": [
    "神经网络由对数据执行操作的层/模块组成。torch.nn命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch中的每个模块都是 nn.Module 的子类。神经网络是一个模块本身，它由其他模块（层）组成。这种嵌套结构允许轻松构建和管理复杂的架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ff9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05ca05",
   "metadata": {},
   "source": [
    "### Get Device for Training\n",
    "### 获得训练设备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87143733",
   "metadata": {},
   "source": [
    "检查一下 torch.cuda是否可用，否则我们继续使用CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32047d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948b792",
   "metadata": {},
   "source": [
    "### Define the Class\n",
    "### 定义类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ce38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "通过子类化定义我们的神经网络nn.Module，并在中初始化神经\n",
    "网络层__init__。每个nn.Module子类都在方法中实现对输入数\n",
    "据的操作forward\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56ea057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31a7a3",
   "metadata": {},
   "source": [
    "创建 的实例NeuralNetwork，并将其移动到device，并打印其结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd488ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ae1b2",
   "metadata": {},
   "source": [
    "要使用模型，我们将输入数据传递给它。这将执行模型的forward，以及一些后台操作。不要直接使用model.forward()！\n",
    "\n",
    "在输入上调用模型会返回一个10维张量，其中包含每个类的原始预测值。通过nn.Softmax模块的一个实例来获得预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b44de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0920, 0.1008, 0.1154, 0.1003, 0.0978, 0.0999, 0.0974, 0.0952, 0.1041,\n",
      "         0.0970]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaff8df",
   "metadata": {},
   "source": [
    "### Model Layers\n",
    "### 模型层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410d877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142cb33",
   "metadata": {},
   "source": [
    "### nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41a95c",
   "metadata": {},
   "source": [
    "初始化nn.Flatten 层以将每个 2D 28x28 图像转换为 784 个像素值的连续数组（保持小批量维度（dim=0））"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee57286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2c6ec",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7fe2e",
   "metadata": {},
   "source": [
    "线性层是一个模块，它 使用其存储的权重和偏差对输入应用线性变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f51881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6573c",
   "metadata": {},
   "source": [
    "非线性激活是在模型的输入和输出之间创建复杂映射的原因。它们在线性变换后应用以引入非线性，帮助神经网络学习各种现象。\n",
    "\n",
    "在这个模型中，我们在线性层之间使用nn.ReLU，但是还有其他激活可以在模型中引入非线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43c670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1941,  0.4187,  0.4639,  0.2990, -0.4311, -0.2235,  0.1667, -0.1878,\n",
      "          0.2090, -0.2200, -0.0512,  0.2211, -0.0501, -0.1494, -0.1426, -0.1378,\n",
      "          0.2996,  0.3582, -0.0458,  0.0737],\n",
      "        [ 0.0376,  0.3173,  0.5275,  0.5480, -0.4904, -0.2354,  0.0687, -0.0312,\n",
      "          0.0901, -0.4720, -0.1748,  0.6029,  0.1113, -0.1404, -0.0932,  0.1158,\n",
      "         -0.0167,  0.1138, -0.0973,  0.0955],\n",
      "        [ 0.1081,  0.3534,  0.4773,  0.4659, -0.3959, -0.3443,  0.0204, -0.2425,\n",
      "          0.2798, -0.8940, -0.3600,  0.4387,  0.1209, -0.1381, -0.3003,  0.0804,\n",
      "          0.2476, -0.0169, -0.2428, -0.3173]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.4187, 0.4639, 0.2990, 0.0000, 0.0000, 0.1667, 0.0000, 0.2090,\n",
      "         0.0000, 0.0000, 0.2211, 0.0000, 0.0000, 0.0000, 0.0000, 0.2996, 0.3582,\n",
      "         0.0000, 0.0737],\n",
      "        [0.0376, 0.3173, 0.5275, 0.5480, 0.0000, 0.0000, 0.0687, 0.0000, 0.0901,\n",
      "         0.0000, 0.0000, 0.6029, 0.1113, 0.0000, 0.0000, 0.1158, 0.0000, 0.1138,\n",
      "         0.0000, 0.0955],\n",
      "        [0.1081, 0.3534, 0.4773, 0.4659, 0.0000, 0.0000, 0.0204, 0.0000, 0.2798,\n",
      "         0.0000, 0.0000, 0.4387, 0.1209, 0.0000, 0.0000, 0.0804, 0.2476, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07e6cd",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d26607",
   "metadata": {},
   "source": [
    "nn.Sequential是一个有序的模块容器。数据按照定义的顺序通过所有模块。您可以使用顺序容器来组合一个快速网络，例如seq_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acde9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be292d",
   "metadata": {},
   "source": [
    "### nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cca7a",
   "metadata": {},
   "source": [
    "神经网络的最后一个线性层返回logits - [-infty, infty] 中的原始值 - 被传递给 nn.Softmax模块。logits 被缩放为值 [0, 1]，表示模型对每个类别的预测概率。dim参数指示值必须总和为 1 的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0101ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99501ce",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "## 模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23459a8",
   "metadata": {},
   "source": [
    "神经网络内的许多层都是参数化的，即具有在训练期间优化的相关权重和偏差。子类nn.Module化会自动跟踪模型对象中定义的所有字段，并使用模型parameters()或named_parameters()方法使所有参数都可以访问。\n",
    "\n",
    "在此示例中，我们遍历每个参数，并打印其大小和其值的预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b2e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0038, -0.0207,  0.0013,  ...,  0.0263, -0.0162,  0.0270],\n",
      "        [ 0.0308,  0.0055, -0.0062,  ..., -0.0139, -0.0071, -0.0236]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0227], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 4.3909e-02, -3.9007e-02, -1.6041e-05,  ..., -4.0481e-02,\n",
      "          3.9038e-02, -4.4171e-02],\n",
      "        [ 2.9929e-02, -1.2821e-02,  9.1028e-03,  ..., -2.2743e-02,\n",
      "         -3.6408e-02, -3.8682e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0431, -0.0329], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0354,  0.0157,  0.0030,  ...,  0.0074, -0.0229,  0.0057],\n",
      "        [-0.0406, -0.0358,  0.0055,  ...,  0.0011, -0.0352, -0.0408]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0371, -0.0038], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c52ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
